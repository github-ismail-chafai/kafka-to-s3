= Publishing data to Kafka and then streaming the data from Kafka to S3 using Kafka Connect
 

   
{nbsp} +
{nbsp} +

*Description* 

A Scala App writes avro records to kafka using Kafka Schema Registry, then we use Kafka Connect to read data from Kafka and sink the data in S3. We use MinIO docker image to mock S3. 

{nbsp} +



1. First publish the image of your Kafka producer
+
[source,bash]
----
sbt docker:publishLocal
----
and make sure the image was published
+
[source,bash]
----
docker images | grep "producer-app"
----


2. Bring the Docker Compose up
+
[source,bash]
----
docker-compose up -d
----
3. Look at the Kafka-producer/Schema-registry integration
+
[source,bash]
----
curl -s "http://localhost:8081/subjects/test-value/versions/latest" | jq
----
4. Make sure everything is up and running
+
[source,bash]
----
$ docker-compose ps
     Name                  Command               State                    Ports
---------------------------------------------------------------------------------------------
kafka            /etc/confluent/docker/run     Up             0.0.0.0:9092->9092/tcp
kafka-connect    bash -c #                     Up (healthy)   0.0.0.0:8083->8083/tcp, 9092/tcp
                 echo "Installing ...
kafka-producer   /opt/docker/bin/producer-app  Up             8080/tcp
schema-registry  /etc/confluent/docker/run     Up             0.0.0.0:8081->8081/tcp
zookeeper        /etc/confluent/docker/run     Up             2181/tcp, 2888/tcp, 3888/tcp

----

5. Create the S3 Sink connector
+
[source,javascript]
----
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/sink-s3-test/config \
    -d '
 {
        "connector.class": "com.wepay.kafka.connect.bigquery.BigQuerySinkConnector",
        "key.converter":"org.apache.kafka.connect.storage.StringConverter",
        "tasks.max": "1",
        "topics": "test2",
        "sanitizeTopics" : "true",
        "autoCreateTables" : "true",
        "autoUpdateSchemas" : "true",
        "schemaRetriever" : "com.wepay.kafka.connect.bigquery.retrieve.IdentitySchemaRetriever",
        "schemaRegistryLocation":"http://localhost:8081",
        "bufferSize": "100000",
        "maxWriteSize":"10000",
        "tableWriteWait": "100",
        "transforms":"filterExample1",
        "transforms.filterExample1.type":"io.confluent.connect.transforms.Filter$Value",
        "transforms.filterExample1.filter.condition":"$.[?(@.samplingField < 15)]",
        "transforms.filterExample1.filter.type":"include",
        "transforms.filterExample1.missing.or.null.behavior":"fail",
        "transforms": "AddMetadata",
        "transforms.AddMetadata.type": "org.apache.kafka.connect.transforms.InsertField$Value",
        "transforms.AddMetadata.offset.field": "offset",
        "transforms.AddMetadata.partition.field": "partition",
        "project" : "teads-sandbox-146410",
        "defaultDataset" : "sandbox_ssp_test",
        "allowNewBigQueryFields": "true",
        "allowBigQueryRequiredFieldRelaxation": "true",
        "keyfile" : "/root/bq-service-account.json"
        }
'
----

6. You can check the output of kafka-connect by browsing, using `login: minioadmin` and `password: minioadmin` 
+
[source,bash]
----
http://localhost:9000/
----    
+

7. When done testing, clean the stack
+
[source,bash]
----
docker-compose down -v --rmi all --remove-orphans
----
'''

References

* https://hub.confluent.io[Confluent Hub]
* https://docs.confluent.io/current/connect/kafka-connect-s3/index.html#connect-s3[S3 Sink connector docs]
* https://github.com/confluentinc/demo-scene[Confluent demo scene]